{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Hx0wetCfryjs"
      },
      "outputs": [],
      "source": [
        "!pip install darts\n",
        "!pip install tensorboard\n",
        "!pip install torch torchvision"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from darts import TimeSeries\n",
        "from darts.models import TFTModel, TCNModel\n",
        "from darts.dataprocessing.transformers import Scaler\n",
        "import matplotlib.pyplot as plt\n",
        "from google.colab import drive\n",
        "import os\n",
        "import torch\n",
        "import darts\n",
        "from pytorch_lightning.callbacks import Callback\n",
        "from pytorch_lightning.callbacks import EarlyStopping\n",
        "from torchmetrics import MeanAbsolutePercentageError\n",
        "import time\n",
        "import datetime\n",
        "from sklearn.metrics import r2_score\n",
        "from sklearn.metrics import mean_absolute_percentage_error #MAPE\n",
        "from sklearn.metrics import mean_absolute_error #MAE\n",
        "from sklearn.metrics import mean_absolute_error, mean_squared_error"
      ],
      "metadata": {
        "id": "G0b39XSL72H4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if torch.cuda.is_available():\n",
        "    device = torch.device(\"cuda\")\n",
        "    print(f\"GPU ({torch.cuda.get_device_name(0)}) is available.\")\n",
        "else:\n",
        "    device = torch.device(\"cpu\")\n",
        "    print(\"GPU is not available. Using CPU.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w5BhnYgD74L6",
        "outputId": "a707aed7-8bb6-489d-fa36-e80e88650404"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GPU is not available. Using CPU.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "drive.mount('/content/gdrive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KdCz0Voz78Oy",
        "outputId": "51e343fc-509b-42ff-fa2a-19e4efdae5c2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/gdrive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "os.getcwd()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "0ORaJzXr78nh",
        "outputId": "f9ac7574-ecd7-47f6-dfb0-8d3dcaaa3981"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'/content'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "path = \"/content/gdrive/MyDrive/Artigo TFT/Dados/tucurui.csv\"\n",
        "\n",
        "def read_data(path):\n",
        "  # reads data\n",
        "  data = pd.read_csv(path, delimiter=';', decimal=',').dropna()\n",
        "\n",
        "  #specify study range (past 3 years)\n",
        "  data = data.iloc[-365*3:-1,:]\n",
        "\n",
        "  # some formating\n",
        "  current_date_format = \"%d/%m/%Y\"\n",
        "\n",
        "  data['Data'] = pd.to_datetime(data['Data'], format=current_date_format)\n",
        "\n",
        "  # create time series\n",
        "  prec = data[['Data', 'UPH610010000']].copy()\n",
        "  prec_ts = TimeSeries.from_dataframe(prec, time_col=\"Data\", value_cols=['UPH610010000'],fill_missing_dates=False, freq='D')\n",
        "\n",
        "  vazao= data[['Data', 'VazaoNatural']].copy()\n",
        "  vazao_ts = TimeSeries.from_dataframe(vazao, time_col=\"Data\", value_cols=['VazaoNatural'],fill_missing_dates=True, freq='D')\n",
        "\n",
        "  return prec_ts, vazao_ts\n",
        "\n",
        "def mean_absolute_percentage_error(y_true, y_pred):\n",
        "    return np.mean(np.abs((y_true - y_pred) / y_true)) * 100\n",
        "\n",
        "def symmetric_mean_absolute_percentage_error(y_true, y_pred):\n",
        "    return np.mean(2 * np.abs(y_pred - y_true) / (np.abs(y_true) + np.abs(y_pred))) * 100\n",
        "\n",
        "def nash_sutcliffe_efficiency(y_true, y_pred):\n",
        "    numerator = np.sum((y_true - y_pred) ** 2)\n",
        "    denominator = np.sum((y_true - np.mean(y_true)) ** 2)\n",
        "    return 1 - (numerator / denominator)\n",
        "\n",
        "def mean_absolute_error(y_true, y_pred):\n",
        "    return np.mean(np.abs(y_true - y_pred))\n",
        "\n",
        "def root_mean_squared_error(y_true, y_pred):\n",
        "    return np.sqrt(mean_squared_error(y_true, y_pred))\n",
        "\n",
        "def split_data(train_test, train_val, prec_ts, vazao_ts):\n",
        "  # build train, val, test sets for flow(target) and rain data\n",
        "  flow_train_val, flow_test = vazao_ts.split_before(train_test)\n",
        "  flow_train, flow_val = flow_train_val.split_before(train_val)\n",
        "\n",
        "  prec_train_val, prec_test = prec_ts.split_before(train_test)\n",
        "  prec_train, prec_val = prec_train_val.split_before(train_val)\n",
        "\n",
        "  return flow_train, flow_test, flow_val, prec_train, prec_test, prec_val\n",
        "\n",
        "def data_scaling():\n",
        "    # data scaling\n",
        "    #inicialize scaler on flow data sets\n",
        "    transformer_flow = Scaler()\n",
        "\n",
        "    #transform the data on flow sets\n",
        "    trans_flow_train = transformer_flow.fit_transform(flow_train)\n",
        "    trans_flow = transformer_flow.transform(vazao_ts)\n",
        "    trans_flow_val = transformer_flow.transform(flow_val)\n",
        "    trans_flow_test = transformer_flow.transform(flow_test)\n",
        "\n",
        "    #inicialize scaler on rainfall data sets\n",
        "    transformer_prec = Scaler()\n",
        "\n",
        "    #transform the data on rainfall sets\n",
        "    trans_prec_train = transformer_prec.fit_transform(prec_train)\n",
        "    trans_prec = transformer_prec.transform(prec_ts)\n",
        "    trans_prec_val = transformer_prec.transform(prec_val)\n",
        "    trans_prec_test = transformer_prec.transform(prec_test)\n",
        "\n",
        "    return transformer_flow, transformer_prec, trans_flow_train,trans_flow,trans_flow_val,trans_flow_test,trans_prec_train,trans_prec,trans_prec_val,trans_prec_test\n",
        "\n",
        "def build_samples_df(flow_test):\n",
        "\n",
        "    # Define the starting date\n",
        "    start_date = datetime.datetime(2022, 12, 31)\n",
        "\n",
        "    # Define the window size and stride\n",
        "    window_size = 14\n",
        "    stride = 1\n",
        "\n",
        "    # Convert TimeSeries to a pandas DataFrame\n",
        "    timeseries=flow_test\n",
        "    timeseries_df = timeseries.pd_dataframe()\n",
        "\n",
        "    # Generate the samples\n",
        "    num_samples = len(timeseries_df) - window_size + 1\n",
        "    samples = []\n",
        "    for i in range(num_samples):\n",
        "        sample_start = start_date + datetime.timedelta(days=i)\n",
        "        sample_dates = pd.date_range(start=sample_start, periods=window_size)\n",
        "        if set(sample_dates).issubset(timeseries_df.index):\n",
        "            sample = timeseries_df.loc[sample_dates]\n",
        "            samples.append(TimeSeries.from_dataframe(sample))\n",
        "\n",
        "    df_samples = pd.DataFrame()\n",
        "\n",
        "\n",
        "    for i in range(len(samples)):\n",
        "\n",
        "      valores = (samples[i].values())\n",
        "\n",
        "\n",
        "      date = samples[i].start_time()\n",
        "\n",
        "      temp = pd.DataFrame(valores).T\n",
        "      temp['Date'] = date\n",
        "      temp.set_index('Date', inplace=True)\n",
        "\n",
        "      df_samples = pd.concat([df_samples,temp])\n",
        "    return df_samples, samples\n",
        "\n",
        "def build_backtest_df(backtest, transformer_flow):\n",
        "    df_backtest = pd.DataFrame()\n",
        "    for i in range(len(backtest)):\n",
        "      valores = transformer_flow.inverse_transform(backtest[i]).values()\n",
        "      #valores = backtest1[i].values()\n",
        "      #transformer_flow.inverse_transform\n",
        "      date = backtest[i].start_time()\n",
        "\n",
        "      temp = pd.DataFrame(valores).T\n",
        "      temp['Date'] = date\n",
        "      temp.set_index('Date', inplace=True)\n",
        "\n",
        "      df_backtest = pd.concat([df_backtest,temp])\n",
        "    return df_backtest\n",
        "\n",
        "\n",
        "\n",
        "prec_ts, vazao_ts = read_data(path)\n",
        "flow_train, flow_test, flow_val, prec_train, prec_test, prec_val = split_data(0.8, 0.9,prec_ts, vazao_ts)\n",
        "transformer_flow,transformer_prec, trans_flow_train,trans_flow,trans_flow_val,trans_flow_test,trans_prec_train,trans_prec,trans_prec_val,trans_prec_test = data_scaling()\n",
        "\n",
        "df_samples, samples=build_samples_df(flow_test)\n",
        "\n",
        "configs = {\n",
        "    \"config 1\": {\n",
        "        \"model\": TCNModel,\n",
        "        \"target\": trans_flow_train,\n",
        "        \"past_cov\": None,\n",
        "        \"future_cov\": None,\n",
        "        \"val_series\": trans_flow_val,\n",
        "        \"val_past_cov\": None,\n",
        "        \"val_future_cov\": None,\n",
        "        \"optimizer\": torch.optim.Adam,\n",
        "        \"teste\": trans_flow_test,\n",
        "        \"cov\": False\n",
        "    },\n",
        "    \"config 2\": {\n",
        "        \"model\": TCNModel,\n",
        "        \"target\": trans_flow_train,\n",
        "        \"past_cov\": None,\n",
        "        \"future_cov\": None,\n",
        "        \"val_series\": trans_flow_val,\n",
        "        \"val_past_cov\": None,\n",
        "        \"val_future_cov\": None,\n",
        "        \"optimizer\": torch.optim.RMSprop,\n",
        "        \"teste\": trans_flow_test,\n",
        "        \"cov\": False\n",
        "    },\n",
        "    \"config 3\": {\n",
        "        \"model\": TCNModel,\n",
        "        \"target\": trans_flow_train,\n",
        "        \"past_cov\": None,\n",
        "        \"future_cov\": None,\n",
        "        \"val_series\": trans_flow_val,\n",
        "        \"val_past_cov\": None,\n",
        "        \"val_future_cov\": None,\n",
        "        \"optimizer\": torch.optim.SGD,\n",
        "        \"teste\": trans_flow_test,\n",
        "        \"cov\": False\n",
        "    },\n",
        "\n",
        "    \"config 7\": {\n",
        "        \"model\": TFTModel,\n",
        "        \"target\": trans_flow_train,\n",
        "        \"past_cov\": None,\n",
        "        \"future_cov\": None,\n",
        "        \"val_series\": trans_flow_val,\n",
        "        \"val_past_cov\": None,\n",
        "        \"val_future_cov\": None,\n",
        "        \"optimizer\": torch.optim.Adam,\n",
        "        \"teste\": trans_flow_test,\n",
        "        \"cov\": True\n",
        "    },\n",
        "    \"config 8\": {\n",
        "        \"model\": TFTModel,\n",
        "        \"target\": trans_flow_train,\n",
        "        \"past_cov\": None,\n",
        "        \"future_cov\": None,\n",
        "        \"val_series\": trans_flow_val,\n",
        "        \"val_past_cov\": None,\n",
        "        \"val_future_cov\": None,\n",
        "        \"optimizer\": torch.optim.RMSprop,\n",
        "        \"teste\": trans_flow_test,\n",
        "        \"cov\": True\n",
        "    },\n",
        "    \"config 9\": {\n",
        "        \"model\": TFTModel,\n",
        "        \"target\": trans_flow_train,\n",
        "        \"past_cov\": None,\n",
        "        \"future_cov\": None,\n",
        "        \"val_series\": trans_flow_val,\n",
        "        \"val_past_cov\": None,\n",
        "        \"val_future_cov\": None,\n",
        "        \"optimizer\": torch.optim.SGD,\n",
        "        \"teste\": trans_flow_test,\n",
        "        \"cov\": True\n",
        "    }\n",
        "}\n",
        "\n",
        "\n",
        "my_stopper = EarlyStopping(\n",
        "    monitor= \"val_loss\",\n",
        "    patience=50,\n",
        "    min_delta=0.0001,\n",
        "    mode='min'\n",
        ")\n",
        "pl_trainer_kwargs = {\"callbacks\": [my_stopper]}\n",
        "#\"accelerator\": \"gpu\",\"devices\": -1,\n",
        "metrics_dict = {}"
      ],
      "metadata": {
        "id": "JZILNBdSn_zV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "prec_ts"
      ],
      "metadata": {
        "id": "GM5T2brWdFr2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for config, params in configs.items():\n",
        "    model_class = params[\"model\"]\n",
        "    optimizer_cls = params[\"optimizer\"]\n",
        "\n",
        "    print(config)\n",
        "\n",
        "\n",
        "    #cria modelo\n",
        "    if config in ['config 4', 'config 5', 'config 6','config 1','config 2', 'config 3']:\n",
        "\n",
        "      model = model_class(\n",
        "          input_chunk_length=30,\n",
        "          output_chunk_length=14,\n",
        "          loss_fn=torch.nn.MSELoss(),\n",
        "          #loss = darts.utils.SmapeLoss(),\n",
        "          likelihood=None,\n",
        "          nr_epochs_val_period=1,\n",
        "          pl_trainer_kwargs=pl_trainer_kwargs,\n",
        "          optimizer_cls=optimizer_cls,\n",
        "          optimizer_kwargs={\"lr\": 0.0005},\n",
        "          log_tensorboard=True,\n",
        "          save_checkpoints=True,\n",
        "          force_reset=True,\n",
        "          n_epochs=500\n",
        "          )\n",
        "    else:\n",
        "      model = model_class(\n",
        "          input_chunk_length=30,\n",
        "          output_chunk_length=14,\n",
        "          loss_fn=torch.nn.MSELoss(),\n",
        "          #loss = darts.utils.SmapeLoss(),\n",
        "          likelihood=None,\n",
        "          nr_epochs_val_period=1,\n",
        "          pl_trainer_kwargs=pl_trainer_kwargs,\n",
        "          optimizer_cls=optimizer_cls,\n",
        "          optimizer_kwargs={\"lr\": 0.0005},\n",
        "          log_tensorboard=True,\n",
        "          save_checkpoints=True,\n",
        "          force_reset=True,\n",
        "          n_epochs=500,\n",
        "          add_relative_index = params[\"cov\"])\n",
        "\n",
        "    inicio = time.time()\n",
        "\n",
        "    #treina modelo\n",
        "    model.fit(\n",
        "        params[\"target\"],\n",
        "        past_covariates=params[\"past_cov\"],\n",
        "        future_covariates=params[\"future_cov\"],\n",
        "        val_series=params[\"val_series\"],\n",
        "        val_past_covariates=params[\"val_past_cov\"],\n",
        "        val_future_covariates=params[\"val_future_cov\"]\n",
        "    )\n",
        "\n",
        "    fim = time.time()\n",
        "    tempo = fim - inicio\n",
        "\n",
        "    print(f\"Training time for config {config}: {tempo} seconds\")\n",
        "\n",
        "    # roda backtest pra validação posterior\n",
        "    backtest = model.historical_forecasts(series=params[\"teste\"],\n",
        "                                      future_covariates = params[\"future_cov\"],\n",
        "                                      past_covariates = params[\"past_cov\"],\n",
        "                                      #num_samples=1,\n",
        "                                      forecast_horizon=14,\n",
        "                                      stride=1,\n",
        "                                      retrain=False,\n",
        "                                      overlap_end=False,\n",
        "                                      last_points_only=False,\n",
        "                                      verbose=False)\n",
        "\n",
        "    df_backtest = build_backtest_df(backtest, transformer_flow)\n",
        "\n",
        "    # Calculate metrics\n",
        "    metricas = []\n",
        "    for i in range(14):\n",
        "      mape = mean_absolute_percentage_error(df_backtest.iloc[:, i], df_samples.iloc[:, i])\n",
        "      smape = symmetric_mean_absolute_percentage_error(df_backtest.iloc[:, i], df_samples.iloc[:, i])\n",
        "      mae = mean_absolute_error(df_backtest.iloc[:, i], df_samples.iloc[:, i])\n",
        "      rmse = root_mean_squared_error(df_backtest.iloc[:, i], df_samples.iloc[:, i])\n",
        "      metricas.append({\"mape\": mape, \"smape\":smape, \"mae\": mae, \"rmse\":rmse})\n",
        "\n",
        "\n",
        "    # Save metrics for the current configuration in the dictionary\n",
        "    #metrics_dict[config] = metricas\n",
        "    metrics_dict[config] = {\"metrics\": metricas, \"tempo\": tempo}"
      ],
      "metadata": {
        "id": "chrUM-tCpKto"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a DataFrame from the metrics dictionary\n",
        "df_metrics = pd.DataFrame(metrics_dict).T\n",
        "\n",
        "# Calculate the average of each metric\n",
        "df_metrics[\"avg_mape\"] = df_metrics[\"metrics\"].apply(lambda x: sum(metric[\"mape\"] for metric in x) / len(x))\n",
        "df_metrics[\"avg_smape\"] = df_metrics[\"metrics\"].apply(lambda x: sum(metric[\"smape\"] for metric in x) / len(x))\n",
        "df_metrics[\"avg_mae\"] = df_metrics[\"metrics\"].apply(lambda x: sum(metric[\"mae\"] for metric in x) / len(x))\n",
        "df_metrics[\"avg_rmse\"] = df_metrics[\"metrics\"].apply(lambda x: sum(metric[\"rmse\"] for metric in x) / len(x))\n",
        "\n",
        "# Drop the \"metrics\" column as it's no longer needed\n",
        "df_metrics.drop(columns=[\"metrics\"], inplace=True)\n",
        "df_metrics[\"model\"] = [configs[config][\"model\"].__name__ for config in df_metrics.index]\n",
        "df_metrics[\"optimizer\"] = [configs[config][\"optimizer\"].__name__ for config in df_metrics.index]\n",
        "\n",
        "# Display the resulting DataFrame\n",
        "df_metrics"
      ],
      "metadata": {
        "id": "tuB9Gr2iFpQS"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}